#cloud-config

manage_etc_hosts: localhost
role: masters
coreos:
  update:
    reboot-strategy: off
  fleet:
    metadata: "role=master,region=fra1"
  units:
    - name: calico-node.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target,etcd2.service
        After=network-online.target

        [Service]
        Slice=machine.slice
        KillMode=mixed
        Restart=always
        TimeoutStartSec=0
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=$public_ipv4
        Environment=IP=$public_ipv4
        Environment=FELIX_FELIXHOSTNAME=$public_ipv4
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_ENDPOINTS=http://127.0.0.1:2379
        ExecStart=/usr/bin/rkt run \
          --inherit-env \
          --stage1-from-dir=stage1-fly.aci \
          --volume=modules,kind=host,source=/lib/modules,readOnly=false \
          --mount=volume=modules,target=/lib/modules \
          --trust-keys-from-https quay.io/calico/node:v0.20.0

        [Install]
        WantedBy=multi-user.target
    - name: docker.service
      command: start
      enable: true
      drop-ins:
        - name: 60-wait-for-flannel-config.conf
          content: |
            [Unit]
            After=flanneld.service
            Requires=flanneld.service
            Restart=always
            Restart=on-failure
    - name: etcd2.service
      command: start
      enable: true
      drop-ins:
        - name: 20-cloudinit.conf
          content: |
            [Service]
            Environment="ETCD_NAME=%H"
            Environment="ETCD_INITIAL_CLUSTER=%H=http://$private_ipv4:2380"
            Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://$private_ipv4:2380"
            Environment="ETCD_ADVERTISE_CLIENT_URLS=http://$private_ipv4:2379"
            Environment="ETCD_LISTEN_PEER_URLS=http://$private_ipv4:2380"
            Environment="ETCD_LISTEN_CLIENT_URLS=http://$private_ipv4:2379,http://127.0.0.1:2379"
    - name: flanneld.service
      command: start
      enable: true
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network": "10.2.0.0/16", "Backend": {"Type": "vxlan"}}'
    - name: kubelet.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/GoogleCloudPlatform/kubernetes
        Requires=network-online.target,etcd2.service,docker.service

        [Service]
        Restart=always
        RestartSec=10
        TimeoutSec=300
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/cni/net.d
        ExecStartPre=/opt/bin/download-k8s-binary
        EnvironmentFile=/etc/environment
        Environment="RKT_RUN_ARGS=--volume var-log,kind=host,source=/var/log --mount volume=var-log,target=/var/log"
        Environment=KUBELET_IMAGE_URL={{ .DigitalOceanConfig.HyperkubeImageURL }}
        Environment=KUBELET_IMAGE_TAG={{ .DigitalOceanConfig.HyperkubeImageTag }}
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --network-plugin-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=cni \
          --register-schedulable=false \
          --allow-privileged=true \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --hostname-override=$private_ipv4 \
          --cluster_dns=10.3.0.10 \
          --cluster_domain=cluster.local \
          --v=2
        ExecStartPost=/opt/bin/kube-post-start

        [Install]
        WantedBy=multi-user.target
    - name: setup-network-environment.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Setup Network Environment
        Documentation=https://github.com/kelseyhightower/setup-network-environment
        Requires=network-online.target
        After=network-online.target

        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
        ExecStartPre=/usr/bin/curl -L -o /opt/bin/setup-network-environment -z /opt/bin/setup-network-environment https://github.com/kelseyhightower/setup-network-environment/releases/download/1.0.1/setup-network-environment
        ExecStartPre=/usr/bin/chmod +x /opt/bin/setup-network-environment
        ExecStart=/opt/bin/setup-network-environment
        RemainAfterExit=yes
        Type=oneshot
    - name: sshd.socket
      command: restart
      runtime: true
      content: |
        [Socket]
        ListenStream=22022
        FreeBind=true
        Accept=yes
write_files:
  - path: /opt/bin/download-k8s-binary
    permissions: 0755
    content: |
      #!/bin/bash
      mkdir -p /opt/bin
      if [ ! -f /opt/bin/kubectl ]; then
        curl -sSL -o /opt/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v{{ .DigitalOceanConfig.KubernetesVersion }}/bin/linux/amd64/kubectl
        chmod +x /opt/bin/kubectl
      fi

      if [ ! -f /opt/bin/helm ]; then
        ## Install Tiller ##
        wget http://storage.googleapis.com/kubernetes-helm/helm-v2.1.3-linux-amd64.tar.gz --directory-prefix=/tmp/
        tar -C /tmp -xvf /tmp/helm-v2.1.3-linux-amd64.tar.gz
        cp /tmp/linux-amd64/helm /opt/bin/helm
        chmod +x /opt/bin/helm
      fi

      if [ ! -f /etc/kubernetes/ssl/ca.pem ]; then
        openssl genrsa -out /etc/kubernetes/ssl/ca-key.pem 2048
        openssl req -x509 -new -nodes -key /etc/kubernetes/ssl/ca-key.pem -days 10000 -out /etc/kubernetes/ssl/ca.pem -subj "/CN=kube-ca"
        openssl genrsa -out /etc/kubernetes/ssl/apiserver-key.pem 2048
        openssl req -new -key /etc/kubernetes/ssl/apiserver-key.pem -out /etc/kubernetes/ssl/apiserver.csr -subj "/CN=kube-apiserver" -config /etc/kubernetes/ssl/openssl.cnf
        openssl x509 -req -in /etc/kubernetes/ssl/apiserver.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out /etc/kubernetes/ssl/apiserver.pem -days 365 -extensions v3_req -extfile /etc/kubernetes/ssl/openssl.cnf
        openssl genrsa -out /etc/kubernetes/ssl/kubelet-key.pem 2048
        openssl req -new -key /etc/kubernetes/ssl/kubelet-key.pem -out /etc/kubernetes/ssl/kubelet.csr -subj "/CN=kube-kubelet"
        openssl x509 -req -in /etc/kubernetes/ssl/kubelet.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out /etc/kubernetes/ssl/kubelet.pem -days 365
        openssl genrsa -out /etc/kubernetes/ssl/worker-key.pem 2048
        openssl req -new -key /etc/kubernetes/ssl/worker-key.pem -out /etc/kubernetes/ssl/worker.csr -subj "/CN=kube-worker"
        openssl x509 -req -in /etc/kubernetes/ssl/worker.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out /etc/kubernetes/ssl/worker.pem -days 365
        openssl genrsa -out /etc/kubernetes/ssl/master-key.pem 2048
        openssl req -new -key /etc/kubernetes/ssl/master-key.pem -out /etc/kubernetes/ssl/master.csr -subj "/CN=kube-admin"
        openssl x509 -req -in /etc/kubernetes/ssl/master.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out /etc/kubernetes/ssl/master.pem -days 365
        chmod 600 /etc/kubernetes/ssl/*-key.pem
        chown root:root /etc/kubernetes/ssl/*-key.pem
      fi
  - path: /opt/bin/kube-post-start
    permissions: 0755
    content: |
      #!/bin/bash
      until $(curl --output /dev/null --silent --head --fail http://127.0.0.1:8080); do printf '.'; sleep 5; done
      /opt/bin/kubectl config set-cluster default-cluster --server="127.0.0.1:8080"
      /opt/bin/kubectl config set-context default-system --cluster=default-cluster --user=default-admin
      /opt/bin/kubectl config use-context default-system
      /opt/bin/kubectl create namespace calico-system
      /opt/bin/kubectl create -R -f /etc/kubernetes/addons
      sudo -i /opt/bin/helm init
      sudo -i /opt/bin/helm search
  - path: /etc/kubernetes/ssl/openssl.cnf
    permissions: 0755
    content: |
      [req]
      req_extensions = v3_req
      distinguished_name = req_distinguished_name
      [req_distinguished_name]
      [ v3_req ]
      basicConstraints = CA:FALSE
      keyUsage = nonRepudiation, digitalSignature, keyEncipherment
      subjectAltName = @alt_names
      [alt_names]
      DNS.1 = kubernetes
      DNS.2 = kubernetes.default
      IP.1 = 10.3.0.1
      IP.2 = $public_ipv4
      IP.3 = $private_ipv4
  - path: /run/systemd/system/etcd.service.d/30-certificates.conf
    permissions: 0644
    content: |
      [Service]
      Environment=ETCD_CA_FILE=/etc/ssl/etcd/certs/ca.pem
      Environment=ETCD_CERT_FILE=/etc/ssl/etcd/certs/etcd.pem
      Environment=ETCD_KEY_FILE=/etc/ssl/etcd/private/etcd.pem
      Environment=ETCD_PEER_CA_FILE=/etc/ssl/etcd/certs/ca.pem
      Environment=ETCD_PEER_CERT_FILE=/etc/ssl/etcd/certs/etcd.pem
      Environment=ETCD_PEER_KEY_FILE=/etc/ssl/etcd/private/etcd.pem
  - path: /etc/kubernetes/ssl/basic_auth.csv
    permissions: 0644
    content: |
      {{ .Password }},{{ .Username }},admin
  - path: /etc/kubernetes/users/known_users.csv
    permissions: 0644
    content: |
      {{ .Password }},kubelet,kubelet
      {{ .Password }},kube_proxy,kube_proxy
      {{ .Password }},system:scheduler,system:scheduler
      {{ .Password }},system:controller_manager,system:controller_manager
      {{ .Password }},system:logging,system:logging
      {{ .Password }},system:monitoring,system:monitoring
      {{ .Password }},system:dns,system:dns
  - path: /etc/kubernetes/cni/net.d/10-calico.conf
    permissions: 0644
    content: |
      {
          "name": "calico",
          "type": "flannel",
          "delegate": {
              "type": "calico",
              "etcd_endpoints": "http://127.0.0.1:2379",
              "log_level": "none",
              "log_level_stderr": "info",
              "hostname": "$public_ipv4",
              "policy": {
                  "type": "k8s",
                  "k8s_api_root": "http://127.0.0.1:8080"
              }
          }
      }
  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: {{ .DigitalOceanConfig.HyperkubeImageURL }}:{{ .DigitalOceanConfig.HyperkubeImageTag }}
          command:
          - /hyperkube
          - apiserver
          - --allow-privileged=true
          - --apiserver-count=1
          - --bind-address=0.0.0.0
          - --insecure-bind-address=127.0.0.1
          - --etcd-servers=http://127.0.0.1:2379
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --insecure_port=8080
          - --advertise-address=$public_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/master.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/master-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/master-key.pem
          - --basic-auth-file=/etc/kubernetes/ssl/basic_auth.csv
          - --token-auth-file=/etc/kubernetes/users/known_users.csv
          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
          - --v=2
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/users
            name: basic-auth-kubernetes
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/users
          name: basic-auth-kubernetes
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        containers:
        - name: kube-controller-manager
          image: {{ .DigitalOceanConfig.HyperkubeImageURL }}:{{ .DigitalOceanConfig.HyperkubeImageTag }}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/master-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --v=2
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 1
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: {{ .DigitalOceanConfig.HyperkubeImageURL }}:{{ .DigitalOceanConfig.HyperkubeImageTag }}
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --kubeconfig=/etc/kubernetes/master-kubeconfig
          - --proxy-mode=iptables
          - --v=2
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs
            - mountPath: /etc/kubernetes/master-kubeconfig
              name: kubeconfig
              readOnly: true
            - mountPath: /etc/kubernetes/ssl
              name: etc-kube-ssl
              readOnly: true
        volumes:
          - name: ssl-certs
            hostPath:
              path: /usr/share/ca-certificates
          - name: kubeconfig
            hostPath:
              path: "/etc/kubernetes/master-kubeconfig"
          - name: etc-kube-ssl
            hostPath:
              path: "/etc/kubernetes/ssl"
  - path: /etc/kubernetes/manifests/policy-controller.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: calico-policy-controller
        namespace: calico-system
      spec:
        hostNetwork: true
        containers:
          # The Calico policy agent.
          - name: k8s-policy-controller
            image: calico/kube-policy-controller:v0.2.0
            env:
              - name: ETCD_ENDPOINTS
                value: "http://127.0.0.1:2379"
              - name: K8S_API
                value: "http://127.0.0.1:8080"
              - name: LEADER_ELECTION
                value: "true"
          # Leader election container used by the policy agent.
          - name: leader-elector
            image: quay.io/calico/leader-elector:v0.1.0
            imagePullPolicy: IfNotPresent
            args:
              - "--election=calico-policy-election"
              - "--election-namespace=calico-system"
              - "--http=127.0.0.1:4040"
  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: {{ .DigitalOceanConfig.HyperkubeImageURL }}:{{ .DigitalOceanConfig.HyperkubeImageTag }}
          command:
          - /hyperkube
          - scheduler
          - --leader-elect=true
          - --master=http://127.0.0.1:8080
          - --v=2
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 1
  - path: /etc/kubernetes/addons/kube-dns-service.yaml
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Service
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "KubeDNS"
      spec:
        selector:
          k8s-app: kube-dns
        clusterIP: 10.3.0.10
        ports:
        - name: dns
          port: 53
          protocol: UDP
        - name: dns-tcp
          port: 53
          protocol: TCP
  - path: /etc/kubernetes/addons/kube-dns-deployment.yaml
    permissions: 0644
    content: |
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: kube-dns-v20
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          version: v20
          kubernetes.io/cluster-service: "true"
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: kube-dns
            version: v20
        template:
          metadata:
            labels:
              k8s-app: kube-dns
              version: v20
              kubernetes.io/cluster-service: "true"
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
          spec:
            containers:
            - name: kubedns
              image: gcr.io/google_containers/kubedns-amd64:1.8
              resources:
                # TODO: Set memory limits when we've profiled the container for large
                # clusters, then set request = limit to keep this container in
                # guaranteed class. Currently, this container falls into the
                # "burstable" category so the kubelet doesn't backoff from restarting it.
                limits:
                  memory: 170Mi
                requests:
                  cpu: 100m
                  memory: 70Mi
              livenessProbe:
                httpGet:
                  path: /healthz-kubedns
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 8081
                  scheme: HTTP
                # we poll on pod startup for the Kubernetes master service and
                # only setup the /readiness HTTP server once that's available.
                initialDelaySeconds: 3
                timeoutSeconds: 5
              args:
              # command = "/kube-dns"
              - --domain=cluster.local
              - --dns-port=10053
              - --kubecfg-file=/etc/kubernetes/worker-kubeconfig
              ports:
              - containerPort: 10053
                name: dns-local
                protocol: UDP
              - containerPort: 10053
                name: dns-tcp-local
                protocol: TCP
              volumeMounts:
              - mountPath: /etc/kubernetes/worker-kubeconfig
                name: kubeconfig
                readOnly: true
            - name: dnsmasq
              image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
              livenessProbe:
                httpGet:
                  path: /healthz-dnsmasq
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - --cache-size=1000
              - --no-resolv
              - --server=127.0.0.1#10053
              - --log-facility=-
              ports:
              - containerPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                name: dns-tcp
                protocol: TCP
            - name: healthz
              image: gcr.io/google_containers/exechealthz-amd64:1.2
              resources:
                limits:
                  memory: 50Mi
                requests:
                  cpu: 10m
                  memory: 50Mi
              args:
              - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
              - --url=/healthz-dnsmasq
              - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
              - --url=/healthz-kubedns
              - --port=8080
              - --quiet
              ports:
              - containerPort: 8080
                protocol: TCP
            volumes:
            - name: kubeconfig
              hostPath:
                path: /etc/kubernetes/worker-kubeconfig
            dnsPolicy: Default  # Don't use cluster DNS.
  - path: /etc/kubernetes/master-kubeconfig
    permissions: 0644
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/kubelet.pem
          client-key: /etc/kubernetes/ssl/kubelet-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
